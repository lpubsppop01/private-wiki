# Coursera Machine Learning コースの学習メモ１週目
```
Courseraの機械学習コースを受講中。
内容をある程度自分の言葉で説明できるかチェックするメモ。

2017/03/07
機械学習: 古典的・感覚的な定義とモダン・形式的な定義がある。前者は「明示的なプログラミングなしで学習する能力をコンピュータに与える研究分野」という感じ。後者は長いので原文で。教師あり学習と教師なし学習に大別される。

教師あり学習: さらに回帰問題と分類問題に分かれる。回帰問題はサンプル値を連続する関数に当てはめて未知の値を予測するもの。分類問題はサンプル値を個別のカテゴリに分類して未知の値のカテゴリを予測するもの。どちらも正解を与えているから教師あり学習。

教師なし学習: データセットを与えてそこから構造を見つけ出すもの。例示されたのはクラスタリングとカクテルパーティ問題。

2017/03/08
（線形回帰のモデル表現について）
訓練セット training set
サンプル数 m
入力 input x(i) または単に x
出力 output y(i) または単に y
訓練サンプル training sample (x(i), y(i))
仮説 hypothesis h
教師あり学習: 訓練セットを学習アルゴリズムに渡して仮説を得ること。
線形回帰: 仮説を一次関数とするモデル。単回帰（変数がひとつ）とも。
  h_theta(x) = theta_0 + theta_1 * x
  または簡略に h(x) と表記。

2017/03/09
（線形回帰における目的関数について）
モデルのパラメータ: 仮説 h_theta(x) = theta_0 + theta_1 * x の theta_0 と theta_1
データに適合するようにパラメータを算出する。
線形回帰においては最小化問題（minimize と表記）を解くことに相当する。
訓練サンプルの y との差の二乗の総和を最小にする。
（正確な表記ではないけど）
J(theta_0, theta_1) = 1/2m * \sum_{i=1}^m (h_theta(x^i) - y^i)^2
    minimize     J(theta_0, theta_1)
theta_0, theta_1
この J(theta_0, theta_1) を目的関数 cost function という。
この場合は二乗誤差関数 squared error function とか二乗誤差目的関数とも。

2017/03/11
（目的関数の直観的な理解について１）
簡略化して h_theta(x) = theta_1 * x としたとき
仮説関数 h_theta(x) は x に対する関数
目的関数 J(theta_1) は theta_1 に対する関数
J(theta_1) を theta_1 をプロットすると二次曲線になり、
J(theta_1) を最小化する theta_1 = 1 がちょうど二次曲線の下端点に当たることが見てとれる

（目的関数の直観的な理解について２）
今度は簡略化していない h_theta(x) = theta_0 + theta_1 * x で考えると、
J(theta_0, theta_1) は弓なりの曲面になる。
しかしこのビデオでは等高線図を使用する。たぶん目的関数Jを可視化するのにはより便利な方法。
しかし可視化してよい値を読み取るようなやり方は高次では通用しない。可視化が困難であるため。
最適なパラメータを自動的に求めるソフトウェアが望ましい。

（最急降下法 Gradient descent algorithm）
汎用的なアルゴリズムで、線形回帰以外でも使われる。
パラメータの数は３個以上でもOK。
まず初期値（一般には theta_0 = 0、theta_1 = 0）を与える。
そして少しずつ変え続けて J を現象させられないか試していく。
周囲を見て最良の方向に移動し続け、いずれ最小値か、局所的最小値に到達する。
開始地点によってたどり着く局所的最小値が異なる。
数式で表記すると
theta_j := theta_j - α(∂/∂theta_j)J(theta_0, theta_1) (for j = 0 and j = 1)
α: 学習率 learning rate
「:=」: 代入。theta_0 と theta_1 は同時更新（Simultaneous update）。

（最急降下法の直観的理解）
学習率αと導関数項について。
偏微分と常微分。確かに違いは忘れた。
微分ということは接線の勾配ということになる。
接線の勾配に沿って学習率だけ降下することを繰り返す、ということ。
学習率が小さすぎると実行に時間がかかる。
学習率が大きすぎると最小値を通り過ぎて収束しない場合が出てくる。
既に局所的最小値に至っている場合は微分が 0 になるので変化しない。
最小値に近づくにつれ微分は 0 に近づくため、学習率は動的に変更させる必要はない。

（線形回帰に対する最急降下法）
偏微分を解くには解析学が必要だが、なくても式をそのまま利用すれば OK。
線形回帰の場合のコスト関数（目的関数より英文表記に近い）は必ず弓型になる。凸型関数といわれる。
このため局所的最適解は常にグローバルな最適解になる。
バッチ再急降下法ともいわれる。各ステップですべてのトレーニング見本を見ていることから。
よい名称ではないが慣例的に機械学習分野の人は呼ぶ。
繰り返しでない手法（正規方程式と訳されていた）もあるが、大きなデータセットに対しては再急降下法の方がよくスケールする。

（線形代数の復習）
さすがに行列やベクトル関連は普段から使っているので気になったところだけ。
R4x2は 4x2 行列の集合。R4は四次元ベクトルの集合。
mxn 行列と nxo 行列を掛け合わせると mxo 行列が得られる。
順番を変えられることを可換である、可換性をもつという。行列と行列の掛け算は可換ではない。
ただし結合則（どちらから計算してもOK）は満たす。
単位行列 Identity Matrix は通常 I で表す。
逆行列がない行列を特異行列または縮退行列という。
```

------------------------------------------------------------------------

Converted from Evernote content created at 2017-03-07T23:54:58+09:00