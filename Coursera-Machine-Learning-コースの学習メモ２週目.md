# Coursera Machine Learning コースの学習メモ２週目
```
2017/03/15
Octave のインストールまでは土曜日に終わらせておいた。
今回は多変数の線形回帰について。
変数は x_1, x_2, ... で表記。値はこれまで通り y。
フィーチャー（feature）の数を n で表す。
x^(i) とすると i 番目の訓練サンプルの値セットを表す（ベクトルっぽい縦に並べた４値で表記していた）。
x^(i)_j で i 番目のサンプルの j 番目の feature を表す。
j はこれまででいう theta の添え字に相当する。
仮説は h_theta(x) = theta_0 + theta_1 * x_1 + theta_2 * x_2 + ... + theta_n * x_n と表記される。
x, theta は R^(n+1) に属する。
より簡略に転置行列を使って h_theta(x) = theta^T * x とも書ける。
多変数の線形回帰（multiple variable linear regression）は
多変量の線形回帰（multivariate linear regression）とも呼ばれる。
（転置行列を使った表記のことを差しているのだと思う。）

以上を最急降下法に適用する。
コスト関数を J(theta) として式自体は変わらない。
偏微分の結果を踏まえると以下の式になる。
n >= 1
theta_j := theta_j - α/m * Σ^m_(i=1) (h_theta(x^(i)) - y^(i)) * x^(i)_j
(simultaneously update theta_j for j = 0,...,n)
偏微分部分が直観では理解できないが、１変数の線形回帰と言っていることが同じというのはわかる。

2017/03/20
最急降下法の実践的なトリック。フィーチャースケーリング。
異なるフィーチャーが近いスケールにある（似たような値を取りやすい）とき、収束が早まる。
スケールが遠い（ベッドルーム数と敷地面積のようにかなり差がある）場合、
コスト関数の等高線は歪みの大きい楕円になり、振動（前に行ったり後ろに行ったり）して大域的な収束値になかなかたどり着かない。
この対策として、フィーチャーをスケールする。具体的には x_1 と x_2 を -1 から 1 間にすることで収束を早める（数字自体は何でもいい）。
ある程度近ければ（経験的には 1/3 から 3  倍までなら）スケールが異なっても問題ないし、気にしなくていい。
他の手法として平均ノーマライゼーション（正規化？、mean normalization）がある。平均値付近の値を x_i から引く。
先のフィーチャースケーリングと合わせると（というか合わせてこう呼ぶのか？）、
x_i の平均値 μ_i を x_i から引き、範囲（最大値 - 最小値）s_i で割った値 (x_i - μ_i) / s_i を  x_i と置き換える。
収束を早めるのが目的なので、各値は厳密でなくていい。

最急降下法の実践的なチップス。学習率の選び方、デバッグ方法。
イテレーションに合わせてコスト関数をプロットすることで本当に収束しているかを判断できる。
また自動収束テストを作ることも可能だが、閾値の設定が難しいため頼り切りにはできない。

使うフィーチャーの選択について。特に多項式回帰 Polynomial Reguression。
例えば家のサイズに対する価格の変化を３次多項式で表そうとしたとき、
x_1 をサイズ、x_2 をサイズの２乗、x_3 をサイズの３乗とする（このときフィーチャースケーリングの重要度が高まる）。
例示の最後にはルート関数？を使っていた。x が大きくなっても落ちないし、３次のように極大にも向かわない。
こうなるとフィーチャーの選び方が多岐にわたるが、どのフィーチャーを使うかを自動的に判断するアルゴリズムも後半で紹介する。

多変量の線形回帰について要約５点。こういうの日本語でも苦手だ。
・多変数の線形回帰の仮説関数は行列を用いて h_theta(x) = theta^T * T と表せる。
　The hypothesis of multivariable linear regresssion is represented as h_theta(x) = theta^T * T.
・多変数の最急降下法のアルゴリズムを確認した。１値の最急降下法に近い形式で表せる。
　The gradient descent of multivariable linear regression is represented as match the same form of the gradient descent of single variable linear regression.
・フィーチャースケーリングにより最急降下法の収束を早められる。
　We can speed up gradient descent by feature scaling and mean normalization.
・学習率の選び方、動作確認はイテレーションに対するコスト関数の変化をプロットするのがよい。
　Plot with number of iteration on the x-axis and J on the y-axis helps us to debug gradient descent.
・線形回帰でフィットしないデータに対しては多項式回帰を利用することでよりデータにフィットさせることができる。
　We can use the polynomial regression to fit data which linear regression does not fit well.
ポストした後、他の人の回答が見られるのだが、
ひとつしたの ikeda manami 氏が IKEDA MANAMI と回答していたのが…。他はだいたいまじめに書いている感じ。

正規方程式 normal equation。
パラメータ theta を求める方法、つまりここまで最急降下法を使ってきた部分を代替する他の手法。
コスト関数を最小にする方法を解析的に解く場合、微分を取って 0 とする。導出手順は略する。
theta = (X^T * X)^-1 * X^T * y
最急降下法は学習率が必要でありイテレーションを要する。正規法的はどちらも不要。
最急降下法はフィーチャーが増えてもよく機能するが、正規方程式では (X^T * X)^-1 を解く必要があり、
ほとんどの実装においてこれは行列次元の３乗でコストが上昇する。
フィーチャーが 10000 個くらいでどちらを使うか迷う。それより小さいなら正規方程式。使い分け。

X に対して逆行列が取れない典型的な理由は、X の要素に冗長性があるか、またはフィーチャーが多すぎる場合。
フィーチャーを削って是正できる。
```

------------------------------------------------------------------------

Converted from Evernote content created at 2017-03-15T20:20:17+09:00